import numpy as npfrom numpy import linalg as LAfrom scipy.misc import factorialimport model as mdimport utilsimport pickleimport timefrom tqdm import tqdmmodel = md.Model()M = 4N = model.NR = model.Rstored_a_values = np.zeros((M, 1, N))stored_b_values = np.ones((M, N, 1))stored_p_values = np.zeros((M, N, N))def forward_likelihood(i, trace):    N = model.N    likelihoods = np.zeros( (1, N) )    likelihoods[0, 0] = 1    if i != 0:        P = utils.randomization(model.P0, model.uniformization_rate, trace[i][0])        # P = stored_p_values[i, :, :]        likelihoods = np.multiply(            forward_likelihood(i-1, trace).dot(P).dot( model.P1 ),            1# np.transpose( model.M[:, trace[i][1]] )        )        if likelihoods.sum() != 0 :            likelihoods = likelihoods / likelihoods.sum()    return likelihoods## def forward_likelihood_abs(i, trace):#     N = model.N#     likelihoods = np.zeros( (1, N) )#     likelihoods[0, 0] = 1##     if i != 0:#         P = utils.randomization(model.P0, model.uniformization_rate, trace[i][0])#         # P = stored_p_values[i, :, :]#         likelihoods = np.multiply(#             forward_likelihood(i-1, trace).dot(P).dot( model.P1 ),#             1# np.transpose( model.M[:, trace[i][1]] )#         )##     return likelihoodsdef backward_likelihood(i, trace):    N = model.N    M = len( trace )    likelihoods = np.ones((N, 1))    if i < M:        P = utils.randomization(model.P0, model.uniformization_rate, trace[i][0])        # P = stored_p_values[i, :, :]        likelihoods = np.multiply(            P.dot( model.P1 ).dot( backward_likelihood(i+1, trace) ),            model.M[:, trace[i][1]]        )        if likelihoods.sum() != 0:            likelihoods = likelihoods / likelihoods.sum()    return likelihoodsdef backward_likelihood_abs(i, trace):    N = model.N    likelihoods = np.ones((N, 1))    if i < M:        P = utils.randomization(model.P0, model.uniformization_rate, trace[i][0])        # P = stored_p_values[i, :, :]        likelihoods = np.multiply(            P * model.P1 * backward_likelihood_abs(i+1, trace),            model.M[:, trace[i][1]]        )        # likelihoods = P.dot( model.P1 ).dot( backward_likelihood(i+1, trace) )    return likelihoodsdef getSubV(i, l):    return stored_a_values[i, :, :] * LA.matrix_power(model.P0, l)def getSubW(i, l):    return LA.matrix_power( model.P0, l ) * model.P1 * stored_b_values[i, :, :]def getV(i, r):    V = np.zeros(( model.N, r ))    for j in range( r ):        V[:, j] = getSubV(i, j)    return Vdef getW(i, r):    W = np.zeros(( r, model.N ))    for j in range( r ):        W[j, :] = np.transpose( getSubW(i, k-1-j) )    return W########################################################################################################################if __name__ == '__main__':    epsilon = 1    round_counter = 0    while epsilon > 0.18 and round_counter < 18:        # init X0 to a matrix and X1 to a vector        X0_tmp = np.zeros((N, N))        # we can use a vector for X1 according to the fact that we work on MMPP        X1_tmp = np.zeros((N, 1))        Z = np.zeros((N, R))        for trace in model.traces:            M = len(trace)            stored_a_values = np.zeros((M, 1, N))            stored_b_values = np.ones((M, N, 1))            stored_p_values = np.zeros((M, N, N))            X0 = np.zeros( (M, N, N) )            X1 = np.zeros( (M, N, N) )            # stored_a_values = np.zeros((M, 1, N))            # stored_b_values = np.zeros((M, N, 1))            # Store values fot A and B for a faster computation            # print("Computation of forward and backward likelihoods")            for i in range(M):                # stored_p_values[i, :, :] = utils.randomization(model.P0, model.uniformization_rate, trace[i][0])                stored_a_values[i, :, :] = model.forward_likelihood(i, trace) #/forward_likelihood(i, trace).sum()                stored_b_values[i, :, :] = model.backward_likelihood(i+1, trace) #/backward_likelihood(i+1, trace).sum()            system_likelihood_abs = np.float(backward_likelihood_abs(0, trace)[0])            system_likelihood = np.float(backward_likelihood(0, trace)[0])            print( "system likelihood:"+str(system_likelihood_abs ) )            print( "system likelihood_rel:"+str(system_likelihood ) )            for i in tqdm(range( M )):                # use Fox and Glynn [FG88] to determinae l and r                (l, r) = utils.getPoissonBounds(model.uniformization_rate * trace[i][0], .1)                # a is a 1xN vector                a = stored_a_values[i, :, :]                # b is an Nx1 vector                b = stored_b_values[i, :, :]                # Retrive some informations from the model                q = model.uniformization_rate                t = trace[i][0]                # Z row for the observed event should be updated by addin a                Z[:, trace[i][1]] = Z[:, trace[i][1]] + a                for k in range(l, r):                    e_tmp = np.exp(-q * t) * np.power(q * t, k) / factorial(k)                    e = 0                    if e_tmp < np.inf:                        e = e_tmp                    V = getV(i, k)                    W = getW(i, k)                    X0_tmp = X0_tmp + e * V.dot( W )                    X1_tmp = X1_tmp + e * np.multiply(getSubV(i, k), np.transpose(b) )                    # if not i == M-1:                    #     print( np.multiply(e * V.dot(W), model.P0 ) )                    # X1_tmp = X1_tmp + e * np.multiply(np.eye(N, N), np.multiply(getSubV(i, r), b))            X0[i, :, :] = system_likelihood * np.multiply(X0_tmp, model.P0 )            X1[i, :, :] = system_likelihood * np.multiply(X1_tmp, model.P1 )            # X0[i, :, :] = np.multiply(X0_tmp, model.P0 )            # if i == M-1:            #     print( X0_tmp )            # X1[i, :, :] = np.multiply(X1_tmp, model.P1 )        #END MULTIPLE SEQUENCES        Y0 = X0.sum(axis=0)        Y1 = X1.sum(axis=0)        normFactor = Y0.sum(axis=1) + Y1.sum(axis=1)        Y0 = Y0 / normFactor[:, np.newaxis]        Y1 = Y1 / normFactor[:, np.newaxis]        Z = Z / Z.sum(axis=1)[:, np.newaxis]        round_counter = round_counter+1        print("Round: "+ str(round_counter) )        # print("Y0")        # print(Y0)        # print("Y1")        # print(Y1)        # print("Z")        # print(Z)        epsilon = max(            LA.norm( model.P0 - Y0, 2),            LA.norm( model.P1 - Y1, 2),            # LA.norm( model.M - Z, 2)           )        print(epsilon)        model.P0 = np.matrix( Y0 )        model.P1 = np.matrix( Y1 )        model.M = np.matrix( Z )    # EXIT FROM PROCEDURE    # print("Final P0")    # print( model.P0 )    # print("Final P1")    # print( model.P1 )    print("Final M")    print( model.M )    model.compute_generators()    pickle.dump(model, open("saved_models/"+                            "model"+str(N)+"states-"+                            str(time.strftime("%Y%m%d%H%M"))+".p", "wb"))    # D1 = model.P1 * model.uniformization_rate    #    # D0 = model.P0 * model.uniformization_rate - model.uniformization_rate * np.eye(N,N)#*\    #    # print("\nD0:")    # print( D0 )    # print("\nD1:")    # print( D1 )